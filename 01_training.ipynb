{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "289554d1-7dfd-4ffd-b82e-c957a5d77c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.0997, Test Loss: 0.0923\n",
      "Epoch 2/20, Train Loss: 0.0948, Test Loss: 0.0920\n",
      "Epoch 3/20, Train Loss: 0.0943, Test Loss: 0.0924\n",
      "Epoch 4/20, Train Loss: 0.0938, Test Loss: 0.0931\n",
      "Epoch 5/20, Train Loss: 0.0935, Test Loss: 0.0928\n",
      "Epoch 6/20, Train Loss: 0.0931, Test Loss: 0.0930\n",
      "Epoch 7/20, Train Loss: 0.0928, Test Loss: 0.0933\n",
      "Epoch 8/20, Train Loss: 0.0925, Test Loss: 0.0922\n",
      "Epoch 9/20, Train Loss: 0.0922, Test Loss: 0.0928\n",
      "Epoch 10/20, Train Loss: 0.0922, Test Loss: 0.0922\n",
      "Epoch 11/20, Train Loss: 0.0920, Test Loss: 0.0925\n",
      "Epoch 12/20, Train Loss: 0.0918, Test Loss: 0.0927\n",
      "Epoch 13/20, Train Loss: 0.0915, Test Loss: 0.0931\n",
      "Epoch 14/20, Train Loss: 0.0914, Test Loss: 0.0921\n",
      "Epoch 15/20, Train Loss: 0.0912, Test Loss: 0.0925\n",
      "Epoch 16/20, Train Loss: 0.0910, Test Loss: 0.0929\n",
      "Epoch 17/20, Train Loss: 0.0909, Test Loss: 0.0930\n",
      "Epoch 18/20, Train Loss: 0.0907, Test Loss: 0.0938\n",
      "Epoch 19/20, Train Loss: 0.0907, Test Loss: 0.0934\n",
      "Epoch 20/20, Train Loss: 0.0903, Test Loss: 0.0929\n",
      "Model saved to var_lstm_lstm_dense_model20250318.pth\n",
      "Results saved to results_lstm_lstm_dense.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "\n",
    "\n",
    "data = pd.read_csv('master_df.csv')\n",
    "\n",
    "# Convert 'Date' column to datetime\n",
    "data['date_parsed'] = pd.to_datetime(data['Date'], errors='coerce')\n",
    "if data['date_parsed'].isnull().any():\n",
    "    raise ValueError(\"Some dates could not be parsed. Check the date format in the CSV.\")\n",
    "\n",
    "# Define the split date and split the data into Train and Test\n",
    "split_date = pd.to_datetime(\"2021-01-01\")  # adjust as needed\n",
    "train_data = data[data['date_parsed'] < split_date].copy()\n",
    "test_data  = data[data['date_parsed'] >= split_date].copy()\n",
    "\n",
    "# Define feature groups and target\n",
    "ticker_cols = [f\"ticker_data_pre{i}\" for i in range(30, 0, -1)]\n",
    "\n",
    "# 3 assets (SAN, IBE, ITX), each has 5 days => total length = 15\n",
    "other_cols = (\n",
    "    [f\"SAN_pre{i}\" for i in range(5, 0, -1)] +\n",
    "    [f\"IBE_pre{i}\" for i in range(5, 0, -1)] +\n",
    "    [f\"ITX_pre{i}\" for i in range(5, 0, -1)]\n",
    ")\n",
    "\n",
    "# Static features\n",
    "static_cols = [f\"signature_{i}\" for i in range(22)] + [f\"levy_area_{i}\" for i in [12, 13, 14]]\n",
    "\n",
    "# Target column\n",
    "target_col = \"ticker_data_target\"\n",
    "\n",
    "\n",
    "# Extract features function\n",
    "def extract_features(df):\n",
    "    X_ticker = df[ticker_cols].values  # shape: (n_samples, 30)\n",
    "    X_other = df[other_cols].values    # shape: (n_samples, 15)\n",
    "    X_static = df[static_cols].values  # shape: (n_samples, 25)\n",
    "    y = df[target_col].values.reshape(-1, 1)\n",
    "    return X_ticker, X_other, X_static, y\n",
    "\n",
    "X_ticker_train, X_other_train, X_static_train, y_train = extract_features(train_data)\n",
    "X_ticker_test,  X_other_test,  X_static_test,  y_test  = extract_features(test_data)\n",
    "\n",
    "# Scale features\n",
    "scaler_ticker = StandardScaler()\n",
    "X_ticker_train_scaled = scaler_ticker.fit_transform(X_ticker_train)\n",
    "X_ticker_test_scaled  = scaler_ticker.transform(X_ticker_test)\n",
    "\n",
    "scaler_other = StandardScaler()\n",
    "X_other_train_scaled = scaler_other.fit_transform(X_other_train)\n",
    "X_other_test_scaled  = scaler_other.transform(X_other_test)\n",
    "\n",
    "scaler_static = StandardScaler()\n",
    "X_static_train_scaled = scaler_static.fit_transform(X_static_train)\n",
    "X_static_test_scaled  = scaler_static.transform(X_static_test)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_test_scaled  = scaler_y.transform(y_test)\n",
    "\n",
    "\n",
    "# PyTorch Dataset\n",
    "class FinancialDataset(Dataset):\n",
    "    def __init__(self, ticker, other, static, y):\n",
    "        self.ticker = torch.tensor(ticker, dtype=torch.float32)\n",
    "        self.other = torch.tensor(other, dtype=torch.float32)\n",
    "        self.static = torch.tensor(static, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ticker)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.ticker[idx], self.other[idx], self.static[idx], self.y[idx]\n",
    "\n",
    "train_dataset = FinancialDataset(X_ticker_train_scaled, X_other_train_scaled, X_static_train_scaled, y_train_scaled)\n",
    "test_dataset  = FinancialDataset(X_ticker_test_scaled,  X_other_test_scaled,  X_static_test_scaled,  y_test_scaled)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "class VaRLSTMModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 ticker_seq_len=30,   # number of days for ticker\n",
    "                 lstm_hidden=10, \n",
    "                 other_lstm_hidden=4, \n",
    "                 other_seq_len=5,     # length of each other asset sequence\n",
    "                 n_other_assets=3,    # number of other assets\n",
    "                 dense_hidden=8,\n",
    "                 static_dim=25):\n",
    "        super(VaRLSTMModel, self).__init__()\n",
    "        \n",
    "        # Branch 1: LSTM for main ticker\n",
    "        # Input shape (batch, 30, 1)\n",
    "        self.lstm_ticker = nn.LSTM(\n",
    "            input_size=1, \n",
    "            hidden_size=lstm_hidden, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Branch 2: Single LSTM for other assets\n",
    "        # We'll treat the 3 assets as 3 features, each with 5 time steps\n",
    "        # => input_size=3, sequence_length=5\n",
    "        self.lstm_other = nn.LSTM(\n",
    "            input_size=n_other_assets, \n",
    "            hidden_size=other_lstm_hidden, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Branch 3: Dense branch for static features\n",
    "        self.static_branch = nn.Sequential(\n",
    "            nn.Linear(static_dim, dense_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Combined dimension:\n",
    "        combined_in_dim = lstm_hidden + other_lstm_hidden + dense_hidden\n",
    "        \n",
    "        self.combined = nn.Sequential(\n",
    "            nn.Linear(combined_in_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)  # final output\n",
    "        )\n",
    "    \n",
    "    def forward(self, ticker_seq, other_seq, static_features):\n",
    "        \"\"\"\n",
    "        ticker_seq: (batch, 30)\n",
    "        other_seq : (batch, 15) => includes 3 assets, each with 5 days => reshape to (batch, 5, 3)\n",
    "        static_features: (batch, 25)\n",
    "        \"\"\"\n",
    "        # 1. Ticker LSTM\n",
    "        x_ticker = ticker_seq.unsqueeze(-1)  # (batch, 30) => (batch, 30, 1)\n",
    "        _, (h_ticker, _) = self.lstm_ticker(x_ticker)\n",
    "        x_ticker_out = h_ticker[-1]  # (batch, lstm_hidden)\n",
    "        \n",
    "        # 2. Other LSTM\n",
    "        # Reshape from (batch, 15) => (batch, 5, 3)\n",
    "        batch_size = other_seq.shape[0]\n",
    "        x_other = other_seq.view(batch_size, 5, 3)\n",
    "        \n",
    "        _, (h_other, _) = self.lstm_other(x_other)\n",
    "        x_other_out = h_other[-1]  # (batch, other_lstm_hidden)\n",
    "        \n",
    "        # 3. Static branch\n",
    "        x_static = self.static_branch(static_features)  # (batch, dense_hidden)\n",
    "        \n",
    "        # 4. Combine\n",
    "        combined = torch.cat((x_ticker_out, x_other_out, x_static), dim=1)\n",
    "        out = self.combined(combined)  # (batch, 1)\n",
    "        return out\n",
    "\n",
    "# Instantiate model\n",
    "model = VaRLSTMModel(\n",
    "    ticker_seq_len=30, \n",
    "    lstm_hidden=8,\n",
    "    other_lstm_hidden=4,\n",
    "    other_seq_len=5, \n",
    "    n_other_assets=3, \n",
    "    dense_hidden=8, \n",
    "    static_dim=25\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# Quantile loss\n",
    "def quantile_loss(y_pred, y_true, q):\n",
    "    error = y_true - y_pred\n",
    "    return torch.mean(torch.max(q * error, (q - 1) * error))\n",
    "\n",
    "quantile = 0.05  # VaR at 5%\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for ticker_seq, other_seq, static_features, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ticker_seq, other_seq, static_features)\n",
    "        loss = quantile_loss(outputs, batch_y, quantile)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for ticker_seq, other_seq, static_features, batch_y in test_loader:\n",
    "            outputs = model(ticker_seq, other_seq, static_features)\n",
    "            loss = quantile_loss(outputs, batch_y, quantile)\n",
    "            test_loss += loss.item()\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "today_str = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "model_filename = f\"var_lstm_lstm_dense_model{today_str}.pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, model_filename)\n",
    "print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "\n",
    "# Inference on entire dataset\n",
    "X_ticker_all, X_other_all, X_static_all, y_all = extract_features(data)\n",
    "X_ticker_all_scaled = scaler_ticker.transform(X_ticker_all)\n",
    "X_other_all_scaled = scaler_other.transform(X_other_all)\n",
    "X_static_all_scaled = scaler_static.transform(X_static_all)\n",
    "\n",
    "ticker_tensor = torch.tensor(X_ticker_all_scaled, dtype=torch.float32)\n",
    "other_tensor  = torch.tensor(X_other_all_scaled,  dtype=torch.float32)\n",
    "static_tensor = torch.tensor(X_static_all_scaled, dtype=torch.float32)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_preds_scaled = model(ticker_tensor, other_tensor, static_tensor).numpy()\n",
    "\n",
    "all_preds = scaler_y.inverse_transform(all_preds_scaled)\n",
    "\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    \"Ticker\": data['Ticker'],\n",
    "    \"Date\": data[\"Date\"],\n",
    "    \"target\": data[target_col],\n",
    "    \"prediction\": all_preds.flatten()\n",
    "})\n",
    "results_df[\"Set\"] = np.where(data[\"date_parsed\"] < split_date, \"Train\", \"Test\")\n",
    "results_df.to_csv(\"results_lstm_lstm_dense.csv\", index=False)\n",
    "print(\"Results saved to results_lstm_lstm_dense.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dissertation3.8)",
   "language": "python",
   "name": "dissertation3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
